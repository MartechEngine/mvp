File: 5.8-Backend-DCS-Orchestration-Engine.md
Phase: 5 - DCS Engine & Scan Results
Part: 5.8
Title: Enterprise DCS Orchestration Engine
Depends On: 5.7-Backend-Project-Memory-Service.md
Objective: To implement the DCSOrchestrationEngine, a robust, asynchronous workflow manager built on Celery. This service is responsible for orchestrating the entire lifecycle of a DCS scan, including credit management, checking for cached data, executing the resilient API fallback chain, processing data, and saving the final intelligence to Project Memory.
1. Core Principle: Asynchronous, Resilient, and Transactional Workflows
The orchestration engine is designed for long-running, complex tasks that must not fail.
Asynchronous by Default: All scans are executed as background tasks using Celery. The API endpoint that starts a scan returns an immediate 202 Accepted response.
Transactional Integrity: The entire workflow is designed to be transactional. If any critical step fails, the Credit Reservation is safely released, and the Audit record is marked as FAILED. The user is never charged for incomplete work.
Resilient Fallback Chain: The engine explicitly implements the fallback logic from our product vision (DataForSEO -> Custom Crawler -> Vertex AI Estimate).
2. DCS Orchestration Workflow Diagram
sequenceDiagram
    participant API
    participant CeleryTask as DCS Orchestrator (Task)
    participant CreditService
    participant DataForSEO
    participant VertexAI as VertexAI (Analysis & Fallback)
    participant MemoryService
    participant AuditDB as Audits Table
    
    API->>CeleryTask: Start Scan (audit_id)
    CeleryTask->>AuditDB: Update Audit status to RUNNING
    CeleryTask->>CreditService: Reserve Credits
    CreditService-->>CeleryTask: reservation_id
    
    alt Primary Data Source
        CeleryTask->>DataForSEO: Fetch SEO Data
        DataForSEO-->>CeleryTask: Raw SEO Data
    else DataForSEO Fails
        CeleryTask->>VertexAI: Estimate SEO Data (Fallback)
        VertexAI-->>CeleryTask: Estimated SEO Data
    end
    
    CeleryTask->>VertexAI: Analyze Data & Get AI Score
    VertexAI-->>CeleryTask: Structured AI Analysis
    
    CeleryTask->>MemoryService: Update Memory (Save Assets & Scores)
    MemoryService-->>CeleryTask: Success
    
    CeleryTask->>CreditService: Consume Reservation (actual_cost)
    CreditService-->>CeleryTask: Success
    
    CeleryTask->>AuditDB: Update Audit status to COMPLETED

3. Celery Task Implementation
This Celery task encapsulates the entire orchestration logic.
File Location: backend/app/tasks/dcs_tasks.py
File Content:

import logging
from celery import shared_task
from sqlalchemy.orm import Session, joinedload
from typing import Optional

from app.core.database import SessionLocal
from app.core.config import settings
from app.core.exceptions import InsufficientCreditsError
from app.services.credit_service import CreditService
from app.services.project_memory_service import ProjectMemoryService
from app.services.api_clients.dataforseo_client import DataForSEOClient
from app.services.api_clients.pagespeed_client import GooglePageSpeedClient
from app.services.api_clients.vertexai_client import VertexAIClient
from app.models.audit import Audit, AuditStatus, AuditType, ScanResult
from app.models.project import Project
from app.models.project_memory import AssetType
from app.models.credit_ledger import CreditActionType, CreditLedger
from app.core.sentry_utils import track_business_event, add_error_context, sentry_trace

logger = logging.getLogger(__name__)

SCAN_COSTS = {
    AuditType.QUICK_SCAN: 10,
    AuditType.DEEP_SCAN: 75,
}

@shared_task(bind=True, max_retries=3, default_retry_delay=300)
@sentry_trace("dcs_scan_orchestration")
def run_dcs_scan(self, audit_id: str):
    """
    Robust DCS scan orchestrator with guaranteed credit refunds on failure.
    
    This is the core orchestration engine that manages the complete DCS scan lifecycle:
    
    Workflow Phases:
    1. **Initialization**: Load audit data and initialize all required services
    2. **Credit Reservation**: Atomically reserve credits before any work begins
    3. **Data Collection**: Execute resilient fallback chain for SEO/performance data
    4. **AI Analysis**: Process collected data through VertexAI for insights
    5. **Memory Storage**: Save results and generated assets to project memory
    6. **Credit Consumption**: Finalize credit transaction for completed work
    7. **Cleanup**: Update audit status and handle any final operations
    
    Error Handling Strategy:
    - **Transactional Integrity**: All database operations are atomic
    - **Guaranteed Refunds**: Credits are always refunded on any failure
    - **Graceful Degradation**: Fallback chains ensure partial success when possible
    - **Comprehensive Logging**: All failures are tracked with full context
    - **Status Tracking**: Audit records maintain accurate state throughout
    
    Resilience Features:
    - **Database Locking**: Prevents race conditions during credit operations
    - **Service Isolation**: External API failures don't cascade to other components
    - **Retry Logic**: Built-in retry mechanisms for transient failures
    - **Circuit Breaking**: Automatic fallback when primary services are unavailable
    
    Args:
        audit_id (str): UUID of the audit record to process
        
    Raises:
        InsufficientCreditsError: If organization lacks credits for the scan
        ValidationError: If audit data is invalid or incomplete
        Exception: For any other system errors during processing
        
    Note:
        This method implements the complete reserve/consume/refund lifecycle
        as specified in overview.md Section 6.5 and 9.2. All credit operations
        are guaranteed to be atomic and consistent.
    """
    db: Session = SessionLocal()
    reservation: Optional[CreditLedger] = None
    scan_result: Optional[ScanResult] = None
    
    try:
        # Load audit with relationships
        audit = db.query(Audit).options(
            joinedload(Audit.project).joinedload(Project.organization)
        ).filter(Audit.id == audit_id).first()

        if not audit:
            logger.error(f"Audit {audit_id} not found")
            add_error_context("audit_not_found", {"audit_id": audit_id})
            return

        # Initialize services
        credit_service = CreditService(db)
        memory_service = ProjectMemoryService(db)
        dataforseo_client = DataForSEOClient(settings)
        pagespeed_client = GooglePageSpeedClient(settings)
        vertexai_client = VertexAIClient(settings)
        
        # Phase 1: Initialize scan and reserve credits
        try:
            audit.status = AuditStatus.RUNNING
            scan_result = ScanResult(audit_id=audit.id)
            db.add(scan_result)
            db.flush()  # Get scan_result.id
            
            scan_cost = SCAN_COSTS.get(audit.audit_type, 25)
            reservation = credit_service.reserve_credits(
                organization_id=audit.project.organization_id,
                amount=scan_cost,
                action_type=CreditActionType.SCAN,
                description=f"DCS {audit.audit_type.value} scan",
                user_id=audit.created_by_id,
                reference_id=audit_id
            )
            
            track_business_event("dcs_scan_started", {
                "audit_id": audit_id,
                "audit_type": audit.audit_type.value,
                "organization_id": str(audit.project.organization_id),
                "scan_cost": scan_cost,
                "reservation_id": str(reservation.id)
            })
            
            db.commit()  # Commit reservation and scan initialization
            
        except InsufficientCreditsError as e:
            audit.status = AuditStatus.FAILED
            audit.failure_reason = "Insufficient credits"
            db.commit()
            
            track_business_event("dcs_scan_insufficient_credits", {
                "audit_id": audit_id,
                "organization_id": str(audit.project.organization_id),
                "required_credits": scan_cost
            })
            
            logger.warning(f"Insufficient credits for audit {audit_id}: {e}")
            return
        
        # Phase 2: Data collection with resilient fallback chain
        seo_data = None
        perf_data = None
        ai_analysis = None
        
        try:
            # Primary data source: DataForSEO
            try:
                seo_data = await dataforseo_client.get_backlink_summary(audit.project.url)
                logger.info(f"DataForSEO data collected for audit {audit_id}")
            except Exception as e:
                logger.warning(f"DataForSEO failed for audit {audit_id}: {e}")
                add_error_context("dataforseo_fallback", {"audit_id": audit_id, "error": str(e)})
                # Fallback: Use VertexAI estimation (implementation would go here)
                seo_data = {"estimated": True, "source": "vertex_ai_fallback"}

            # Google PageSpeed (with fallback)
            try:
                perf_data = await pagespeed_client.analyze_url(audit.project.url)
                logger.info(f"PageSpeed data collected for audit {audit_id}")
            except Exception as e:
                logger.warning(f"PageSpeed failed for audit {audit_id}: {e}")
                add_error_context("pagespeed_fallback", {"audit_id": audit_id, "error": str(e)})
                perf_data = {"estimated": True, "source": "fallback"}
            
            # AI Analysis (critical step)
            ai_analysis = await vertexai_client.analyze_seo_data({
                "seo_data": seo_data,
                "performance_data": perf_data,
                "url": audit.project.url
            })
            
            if not ai_analysis:
                raise ValueError("AI analysis returned empty result")
                
            logger.info(f"AI analysis completed for audit {audit_id}")
            
        except Exception as e:
            # Data collection failed - refund and fail
            logger.error(f"Data collection failed for audit {audit_id}: {e}")
            add_error_context("data_collection_failure", {"audit_id": audit_id, "error": str(e)})
            
            # Guaranteed refund
            credit_service.refund_reservation(
                reservation_id=str(reservation.id),
                reason=f"Data collection failed: {str(e)[:200]}"
            )
            
            # Update audit status
            audit.status = AuditStatus.FAILED
            audit.failure_reason = f"Data collection failed: {str(e)}"
            db.commit()
            
            track_business_event("dcs_scan_data_collection_failed", {
                "audit_id": audit_id,
                "error": str(e),
                "refund_issued": True
            })
            
            return
        
        # Phase 3: Results persistence
        try:
            # Update scan scores
            memory_service.update_scan_scores(
                scan_result=scan_result,
                dcs_score=ai_analysis.get("dcs_score", 0),
                ai_citation_score=ai_analysis.get("ai_citation_score", 0)
            )
            
            # Save generated assets
            if ai_analysis.get("generated_content"):
                for asset_type, content in ai_analysis["generated_content"].items():
                    memory_service.save_generated_asset(
                        project_id=audit.project_id,
                        scan_result_id=scan_result.id,
                        asset_type=AssetType(asset_type),
                        content=content
            )
            
            logger.info(f"Results persisted for audit {audit_id}")
            
        except Exception as e:
            # Persistence failed - refund and fail
            logger.error(f"Results persistence failed for audit {audit_id}: {e}")
            add_error_context("persistence_failure", {"audit_id": audit_id, "error": str(e)})
            
            # Guaranteed refund
            credit_service.refund_reservation(
                reservation_id=str(reservation.id),
                reason=f"Results persistence failed: {str(e)[:200]}"
            )
            
            # Update audit status
            audit.status = AuditStatus.FAILED
            audit.failure_reason = f"Results persistence failed: {str(e)}"
            db.commit()
            
            track_business_event("dcs_scan_persistence_failed", {
                "audit_id": audit_id,
                "error": str(e),
                "refund_issued": True
            })
            
            return
        
        # Phase 4: Success - consume credits and complete
        try:
            credit_service.consume_reservation(str(reservation.id))
            audit.credits_consumed = scan_cost
            audit.status = AuditStatus.COMPLETED
            db.commit()
            
            track_business_event("dcs_scan_completed", {
                "audit_id": audit_id,
                "audit_type": audit.audit_type.value,
                "organization_id": str(audit.project.organization_id),
                "credits_consumed": scan_cost,
                "dcs_score": ai_analysis.get("dcs_score", 0),
                "ai_citation_score": ai_analysis.get("ai_citation_score", 0)
            })
            
            logger.info(f"DCS scan {audit_id} completed successfully")
            
        except Exception as e:
            # Final step failed - this is critical, refund immediately
            logger.error(f"Credit consumption failed for audit {audit_id}: {e}")
            add_error_context("credit_consumption_failure", {"audit_id": audit_id, "error": str(e)})
            
            # Guaranteed refund
            credit_service.refund_reservation(
                reservation_id=str(reservation.id),
                reason=f"Credit consumption failed: {str(e)[:200]}"
            )
            
            # Update audit status
            audit.status = AuditStatus.FAILED
            audit.failure_reason = f"Credit consumption failed: {str(e)}"
            db.commit()
            
            track_business_event("dcs_scan_finalization_failed", {
                "audit_id": audit_id,
                "error": str(e),
                "refund_issued": True
            })
            
            return
            
    except Exception as exc:
        # Catastrophic failure - ensure refund and proper cleanup
        logger.error(f"Catastrophic failure in DCS scan {audit_id}: {exc}", exc_info=True)
        add_error_context("catastrophic_scan_failure", {"audit_id": audit_id, "error": str(exc)})
        
        try:
            if db.is_active:
                db.rollback()
            
            # Ensure refund in separate transaction
            if reservation:
                refund_db = SessionLocal()
                try:
                    refund_credit_service = CreditService(refund_db)
                    refund_credit_service.refund_reservation(
                        reservation_id=str(reservation.id),
                        reason=f"Catastrophic scan failure: {str(exc)[:200]}"
                    )
                    refund_db.commit()
                    logger.info(f"Emergency refund issued for audit {audit_id}")
                except Exception as refund_error:
                    logger.error(f"Emergency refund failed for audit {audit_id}: {refund_error}")
                finally:
                    refund_db.close()
            
            # Update audit status in separate transaction
            status_db = SessionLocal()
            try:
                failed_audit = status_db.query(Audit).filter(Audit.id == audit_id).first()
                if failed_audit:
                    failed_audit.status = AuditStatus.FAILED
                    failed_audit.failure_reason = f"System error: {str(exc)}"
                    status_db.commit()
            except Exception as status_error:
                logger.error(f"Failed to update audit status for {audit_id}: {status_error}")
            finally:
                status_db.close()
                
            track_business_event("dcs_scan_catastrophic_failure", {
                "audit_id": audit_id,
                "error": str(exc),
                "refund_attempted": reservation is not None
            })
            
        except Exception as cleanup_error:
            logger.error(f"Cleanup failed for audit {audit_id}: {cleanup_error}")
        
        # Retry for transient errors (with exponential backoff)
        if self.request.retries < self.max_retries:
            raise self.retry(exc=exc, countdown=60 * (2 ** self.request.retries))
        else:
            logger.error(f"Max retries exceeded for audit {audit_id}, giving up")
            
    finally:
        if db and db.is_active:
            db.close()

4. Next Steps
This file concludes the core service implementation for the DCS Foundation. We have now defined all the backend components necessary to perform our core intelligence-gathering operations.
Next File: 5.9-Backend-Real-Time-Progress-API.md will define the Server-Sent Events (SSE) architecture needed to provide real-time scan progress updates to the frontend, as required by the product vision.