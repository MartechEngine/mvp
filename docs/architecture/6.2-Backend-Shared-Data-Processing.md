Phase: 6 - Content & Analysis Suite
Part: 6.2
Title: Enterprise Shared Data Processing Utilities
Depends On: 6.1-Backend-Unified-AI-Service.md
Objective: To implement a centralized, high-performance library of shared utilities for text processing, content analysis, and SEO metric calculation. This EnterpriseTextProcessor will be consumed by higher-level services to prepare data for AI analysis and to enrich the results, ensuring consistency, efficiency, and adherence to production best practices.
1. Core Principles: Efficiency, Purity, and Reusability
Efficiency: These utilities will be called thousands of times during a single Deep Scan. They must be highly optimized for speed and low memory usage.
Purity: The functions will be designed as pure functions where possible (input -> output), making them predictable, easy to test, and safe for parallel execution.
Decoupling: This service is a utility library. It is intentionally "dumb" about business logic and does not depend on other complex services like CreditService or ProjectMemoryService. It simply processes the data it's given.
2. Dependency Installation
This service requires several powerful text-processing libraries.
File to Modify: backend/requirements.txt
Content to Add:
beautifulsoup4>=4.12.0
textstat>=0.7.0
langdetect>=1.0.9
nltk>=3.8.0```

### **3. Production Best Practice: Pre-downloading NLTK Data**

A common anti-pattern is for `nltk` to download its data models at runtime. In a production/containerized environment, this is unreliable and can fail. The correct approach is to download this data during the Docker image build process.

**File to Modify:** `backend/Dockerfile`
**Content to Add:**
```dockerfile
# In the Builder stage, after installing requirements:
# --- START MODIFICATION ---
# Pre-download NLTK data to prevent runtime downloads in production
RUN python -m nltk.downloader punkt
# --- END MODIFICATION ---
4. Pydantic Schemas for Processing Results
We define clear data structures for the output of our processing functions.
File Location: backend/app/schemas/processing_schemas.py
File Content:
from pydantic import BaseModel, Field, ConfigDict
from typing import List, Optional, Dict

class ReadabilityMetrics(BaseModel):
    """Schema for various readability scores."""
    flesch_reading_ease: float
    flesch_kincaid_grade: float
    automated_readability_index: float
    reading_time_minutes: int

class ContentStructure(BaseModel):
    """Schema for the structural analysis of HTML content."""
    heading_counts: Dict[str, int] = Field(default_factory=dict) # e.g., {"h1": 1, "h2": 4}
    internal_link_count: int
    external_link_count: int
    image_count: int
    images_without_alt_text: int

class TextAnalysisResult(BaseModel):
    """The comprehensive result of processing a piece of text."""
    clean_text: str
    word_count: int
    sentence_count: int
    paragraph_count: int
    language: Optional[str]
    readability: ReadabilityMetrics
    content_structure: ContentStructure

5. EnterpriseTextProcessor Implementation
This class is a collection of high-performance methods for text and content analysis.
File Location: backend/app/services/shared/text_processor.py
File Content:
import logging
import re
import nltk
from bs4 import BeautifulSoup
from textstat import flesch_reading_ease, flesch_kincaid_grade, automated_readability_index, sentence_count, lexicon_count
from langdetect import detect, LangDetectError
from typing import Dict, Optional, Tuple
from urllib.parse import urlparse

from app.schemas.processing_schemas import ReadabilityMetrics, ContentStructure, TextAnalysisResult

logger = logging.getLogger(__name__)

# NLTK data is pre-downloaded during the Docker build, so this will work without runtime downloads.
nltk.data.path.append("/opt/venv/nltk_data") # Adjust path if necessary for your container setup

class EnterpriseTextProcessor:
    """A high-performance utility service for text and content analysis."""

    def clean_html_to_text(self, html_content: str) -> str:
        """Strips all HTML tags, scripts, styles, and extra whitespace to return clean, readable text."""
        try:
            soup = BeautifulSoup(html_content, 'lxml')
            for element in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):
                element.decompose()
            text = soup.get_text(separator='\n', strip=True)
            text = re.sub(r'\n{3,}', '\n\n', text) # Normalize newlines
            return text
        except Exception:
            return re.sub(r'<[^>]+>', '', html_content) # Basic fallback

    def analyze_readability(self, text: str) -> ReadabilityMetrics:
        """Calculates various readability scores for the given text."""
        word_count = lexicon_count(text, removepunct=True)
        reading_time = max(1, round(word_count / 200)) # Assumes 200 WPM reading speed
        return ReadabilityMetrics(
            flesch_reading_ease=flesch_reading_ease(text),
            flesch_kincaid_grade=flesch_kincaid_grade(text),
            automated_readability_index=automated_readability_index(text),
            reading_time_minutes=reading_time
        )

    def detect_language(self, text: str) -> Optional[str]:
        """Detects the primary language of the text."""
        try:
            if len(text) < 50: return None
            return detect(text)
        except LangDetectError:
            return None

    def analyze_content_structure(self, html_content: str, base_domain: str) -> ContentStructure:
        """Analyzes the HTML structure for SEO-relevant elements."""
        soup = BeautifulSoup(html_content, 'lxml')
        heading_counts = {f"h{i}": len(soup.find_all(f"h{i}")) for i in range(1, 7)}
        
        internal_link_count = 0
        external_link_count = 0
        for a in soup.find_all('a', href=True):
            href = a['href']
            if not href or href.startswith(('#', 'mailto:', 'tel:')):
                continue
            
            parsed_url = urlparse(href)
            if parsed_url.scheme and parsed_url.netloc and base_domain not in parsed_url.netloc:
                external_link_count += 1
            else:
                internal_link_count += 1

        images = soup.find_all('img')
        images_without_alt = [img for img in images if not img.get('alt', '').strip()]
        
        return ContentStructure(
            heading_counts=heading_counts,
            internal_link_count=internal_link_count,
            external_link_count=external_link_count,
            image_count=len(images),
            images_without_alt_text=len(images_without_alt)
        )

    def process_text_from_html(self, html_content: str, base_domain: str) -> TextAnalysisResult:
        """
        Runs a full pipeline of text processing and analysis. This is the main entry point.
        """
        clean_text = self.clean_html_to_text(html_content)
        
        return TextAnalysisResult(
            clean_text=clean_text,
            word_count=lexicon_count(clean_text, removepunct=True),
            sentence_count=sentence_count(clean_text),
            paragraph_count=len([p for p in clean_text.split('\n\n') if p.strip()]),
            language=self.detect_language(clean_text),
            readability=self.analyze_readability(clean_text),
            content_structure=self.analyze_content_structure(html_content, base_domain)
        )

6. Next Steps
With this powerful set of data processing utilities now defined, our higher-level services can offload common analysis tasks, ensuring consistency and performance.
Next File: 6.3-Backend-Content-Generation-Service.md will implement the EnterpriseContentGenerationService. This service will be a primary consumer of our new EnterpriseTextProcessor, using it to analyze reference content and to quality-check the content generated by the AI.