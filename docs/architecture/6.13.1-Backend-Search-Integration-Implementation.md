Phase: 6 - Content & Analysis Suite
Part: 6.13.1
Title: Backend Search Integration Implementation
Depends On: 6.13-Backend-Search-Service.md, 3.2-Backend-Project-API.md, 6.10-Frontend-Content-Hub-UI.md
Objective: To implement comprehensive search functionality across projects, generated content, scan results, and user data. This integration provides fast, relevant search results with faceted filtering, auto-complete suggestions, and semantic search capabilities for enhanced user experience.

## 1. Core Principles

**Performance First**: Fast search responses with proper indexing and caching strategies.
**Relevance Scoring**: Intelligent ranking of search results based on user context and content quality.
**Faceted Search**: Multi-dimensional filtering for precise result refinement.
**Real-time Updates**: Automatic index updates when content is created or modified.

## 2. Enhanced Search Service Implementation

### Comprehensive Search Service
```python
# File: backend/app/services/enhanced_search_service.py
import logging
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import text, func, and_, or_
from datetime import datetime, timedelta
import re

from app.models.search_index import SearchIndex, SearchQuery, SearchableType
from app.models.project import Project
from app.models.generated_content import GeneratedContent
from app.models.scan_result import ScanResult
from app.core.config import settings
from app.core.exceptions import ValidationError
from app.core.sentry_utils import track_business_event, track_performance_issue, sentry_trace
from app.schemas.query_schemas import PaginationParams

logger = logging.getLogger(__name__)

class EnhancedSearchService:
    """Comprehensive search service with full-text, semantic, and faceted search capabilities."""
    
    def __init__(self, db: Session):
        self.db = db
        self.max_results_per_page = 50
        self.default_results_per_page = 20
    
    @sentry_trace("search_comprehensive")
    async def search_comprehensive(self, query_text: str, user_id: str, organization_id: str,
                                 searchable_types: Optional[List[SearchableType]] = None,
                                 filters: Optional[Dict[str, Any]] = None,
                                 pagination: Optional[PaginationParams] = None) -> Dict[str, Any]:
        """Perform comprehensive search across all searchable content."""
        start_time = datetime.utcnow()
        
        # Validate and clean query
        if not query_text or len(query_text.strip()) < 2:
            raise ValidationError("Search query must be at least 2 characters long")
        
        query_text = query_text.strip()
        
        # Set defaults
        if not searchable_types:
            searchable_types = [SearchableType.PROJECT, SearchableType.GENERATED_CONTENT, SearchableType.SCAN_RESULT]
        
        if not pagination:
            pagination = PaginationParams()
        
        # Build search query
        search_query = self._build_search_query(
            query_text=query_text,
            organization_id=organization_id,
            searchable_types=searchable_types,
            filters=filters
        )
        
        # Execute search with pagination
        total_count = search_query.count()
        
        results = search_query.offset(
            (pagination.page - 1) * pagination.per_page
        ).limit(pagination.per_page).all()
        
        # Process results
        processed_results = []
        for result in results:
            processed_result = await self._process_search_result(result, query_text)
            processed_results.append(processed_result)
        
        # Calculate performance metrics
        response_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)
        
        # Track search query
        await self._track_search_query(
            user_id=user_id,
            organization_id=organization_id,
            query_text=query_text,
            filters=filters,
            results_count=total_count,
            response_time_ms=response_time_ms
        )
        
        # Track performance issues
        track_performance_issue(
            operation="comprehensive_search",
            duration_ms=response_time_ms,
            threshold_ms=500
        )
        
        # Calculate pagination metadata
        total_pages = (total_count + pagination.per_page - 1) // pagination.per_page
        
        return {
            "results": processed_results,
            "pagination": {
                "page": pagination.page,
                "per_page": pagination.per_page,
                "total_items": total_count,
                "total_pages": total_pages,
                "has_next": pagination.page < total_pages,
                "has_prev": pagination.page > 1
            },
            "facets": await self._get_search_facets(organization_id, searchable_types, filters),
            "suggestions": await self._get_search_suggestions(query_text, organization_id),
            "response_time_ms": response_time_ms
        }
    
    @sentry_trace("search_projects")
    async def search_projects(self, query_text: str, organization_id: str,
                            filters: Optional[Dict[str, Any]] = None,
                            pagination: Optional[PaginationParams] = None) -> Dict[str, Any]:
        """Search specifically within projects."""
        return await self.search_comprehensive(
            query_text=query_text,
            user_id=None,  # Will be set by caller
            organization_id=organization_id,
            searchable_types=[SearchableType.PROJECT],
            filters=filters,
            pagination=pagination
        )
    
    @sentry_trace("search_content")
    async def search_content(self, query_text: str, organization_id: str,
                           content_types: Optional[List[str]] = None,
                           filters: Optional[Dict[str, Any]] = None,
                           pagination: Optional[PaginationParams] = None) -> Dict[str, Any]:
        """Search specifically within generated content."""
        search_filters = filters or {}
        
        if content_types:
            search_filters["content_types"] = content_types
        
        return await self.search_comprehensive(
            query_text=query_text,
            user_id=None,  # Will be set by caller
            organization_id=organization_id,
            searchable_types=[SearchableType.GENERATED_CONTENT],
            filters=search_filters,
            pagination=pagination
        )
    
    @sentry_trace("search_index_content")
    async def index_content(self, searchable_type: SearchableType, searchable_id: str,
                          organization_id: str, force_reindex: bool = False) -> bool:
        """Index or reindex specific content."""
        try:
            # Check if already indexed
            existing_index = self.db.query(SearchIndex).filter(
                SearchIndex.searchable_type == searchable_type,
                SearchIndex.searchable_id == searchable_id
            ).first()
            
            if existing_index and not force_reindex:
                # Update last indexed timestamp
                existing_index.last_indexed_at = datetime.utcnow()
                self.db.commit()
                return True
            
            # Get content to index
            content_data = await self._extract_content_for_indexing(searchable_type, searchable_id)
            
            if not content_data:
                logger.warning(f"No content found for indexing: {searchable_type}:{searchable_id}")
                return False
            
            # Create or update search index
            if existing_index:
                # Update existing
                existing_index.title = content_data["title"]
                existing_index.content = content_data["content"]
                existing_index.summary = content_data.get("summary")
                existing_index.metadata = content_data.get("metadata", {})
                existing_index.tags = content_data.get("tags", [])
                existing_index.relevance_score = content_data.get("relevance_score", 1.0)
                existing_index.last_indexed_at = datetime.utcnow()
                search_index = existing_index
            else:
                # Create new
                search_index = SearchIndex(
                    searchable_type=searchable_type,
                    searchable_id=searchable_id,
                    organization_id=organization_id,
                    title=content_data["title"],
                    content=content_data["content"],
                    summary=content_data.get("summary"),
                    metadata=content_data.get("metadata", {}),
                    tags=content_data.get("tags", []),
                    relevance_score=content_data.get("relevance_score", 1.0)
                )
                self.db.add(search_index)
            
            # Update search vector for full-text search
            self._update_search_vector(search_index)
            
            self.db.commit()
            
            track_business_event("content_indexed", {
                "searchable_type": searchable_type,
                "searchable_id": searchable_id,
                "organization_id": organization_id,
                "reindex": force_reindex
            })
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to index content {searchable_type}:{searchable_id}: {e}")
            self.db.rollback()
            return False
    
    def _build_search_query(self, query_text: str, organization_id: str,
                           searchable_types: List[SearchableType],
                           filters: Optional[Dict[str, Any]] = None):
        """Build SQLAlchemy search query with full-text search and filters."""
        
        # Base query
        query = self.db.query(SearchIndex).filter(
            SearchIndex.organization_id == organization_id,
            SearchIndex.is_active == True,
            SearchIndex.searchable_type.in_(searchable_types)
        )
        
        # Full-text search
        if query_text:
            # Create tsquery for PostgreSQL full-text search
            tsquery = self._create_tsquery(query_text)
            
            # Search in title (higher weight) and content
            query = query.filter(
                or_(
                    SearchIndex.search_vector.op('@@')(text(f"to_tsquery('english', '{tsquery}')")),
                    SearchIndex.title.ilike(f"%{query_text}%"),
                    SearchIndex.content.ilike(f"%{query_text}%")
                )
            )
        
        # Apply filters
        if filters:
            query = self._apply_search_filters(query, filters)
        
        # Order by relevance
        if query_text:
            # Calculate relevance score
            relevance_expr = (
                func.ts_rank(SearchIndex.search_vector, text(f"to_tsquery('english', '{tsquery}')")) * 0.4 +
                SearchIndex.relevance_score * 0.3 +
                SearchIndex.popularity_score * 0.2 +
                SearchIndex.recency_score * 0.1
            )
            query = query.order_by(relevance_expr.desc(), SearchIndex.created_at.desc())
        else:
            query = query.order_by(SearchIndex.created_at.desc())
        
        return query
    
    def _apply_search_filters(self, query, filters: Dict[str, Any]):
        """Apply filters to search query."""
        
        # Date range filter
        if "date_from" in filters:
            query = query.filter(SearchIndex.created_at >= filters["date_from"])
        
        if "date_to" in filters:
            query = query.filter(SearchIndex.created_at <= filters["date_to"])
        
        # Tags filter
        if "tags" in filters and filters["tags"]:
            for tag in filters["tags"]:
                query = query.filter(SearchIndex.tags.op('@>')([tag]))
        
        # Metadata filters
        if "metadata" in filters:
            for key, value in filters["metadata"].items():
                query = query.filter(SearchIndex.metadata[key].astext == str(value))
        
        # Content type filter (for generated content)
        if "content_types" in filters and filters["content_types"]:
            query = query.filter(SearchIndex.metadata["content_type"].astext.in_(filters["content_types"]))
        
        # Project filter
        if "project_ids" in filters and filters["project_ids"]:
            query = query.filter(SearchIndex.metadata["project_id"].astext.in_(filters["project_ids"]))
        
        return query
    
    def _create_tsquery(self, query_text: str) -> str:
        """Create PostgreSQL tsquery from search text."""
        # Clean and prepare query for tsquery
        words = re.findall(r'\w+', query_text.lower())
        
        if not words:
            return ""
        
        # Join words with AND operator for tsquery
        return " & ".join(words)
    
    async def _extract_content_for_indexing(self, searchable_type: SearchableType, 
                                          searchable_id: str) -> Optional[Dict[str, Any]]:
        """Extract content data for indexing based on searchable type."""
        
        if searchable_type == SearchableType.PROJECT:
            return await self._extract_project_content(searchable_id)
        elif searchable_type == SearchableType.GENERATED_CONTENT:
            return await self._extract_generated_content(searchable_id)
        elif searchable_type == SearchableType.SCAN_RESULT:
            return await self._extract_scan_result_content(searchable_id)
        else:
            return None
    
    async def _extract_project_content(self, project_id: str) -> Optional[Dict[str, Any]]:
        """Extract project content for indexing."""
        project = self.db.query(Project).filter(Project.id == project_id).first()
        
        if not project:
            return None
        
        # Calculate relevance based on project activity
        scan_count = len(project.audits)
        relevance_score = min(1.0 + (scan_count * 0.1), 2.0)
        
        return {
            "title": project.name,
            "content": f"{project.description or ''} {project.url}",
            "summary": f"Project: {project.name} - {project.url}",
            "metadata": {
                "project_id": str(project.id),
                "url": project.url,
                "scan_count": scan_count,
                "created_at": project.created_at.isoformat(),
                "updated_at": project.updated_at.isoformat()
            },
            "tags": ["project", "website"],
            "relevance_score": relevance_score
        }
    
    async def _process_search_result(self, search_index: SearchIndex, query_text: str) -> Dict[str, Any]:
        """Process and format search result."""
        
        # Generate snippet with highlighted terms
        snippet = self._generate_snippet(search_index.content or "", query_text)
        
        return {
            "id": str(search_index.id),
            "type": search_index.searchable_type,
            "searchable_id": search_index.searchable_id,
            "title": search_index.title,
            "snippet": snippet,
            "metadata": search_index.metadata or {},
            "tags": search_index.tags or [],
            "relevance_score": search_index.relevance_score,
            "created_at": search_index.created_at.isoformat(),
            "last_indexed_at": search_index.last_indexed_at.isoformat()
        }
    
    def _generate_snippet(self, content: str, query_text: str, max_length: int = 200) -> str:
        """Generate snippet with query terms highlighted."""
        if not content or not query_text:
            return content[:max_length] + "..." if len(content) > max_length else content
        
        # Find the best position to show the snippet
        query_words = re.findall(r'\w+', query_text.lower())
        
        best_position = 0
        best_score = 0
        
        # Look for the position with the most query words
        for i in range(0, len(content) - max_length, 50):
            snippet_part = content[i:i + max_length].lower()
            score = sum(1 for word in query_words if word in snippet_part)
            
            if score > best_score:
                best_score = score
                best_position = i
        
        # Extract snippet
        snippet = content[best_position:best_position + max_length]
        
        # Add ellipsis if needed
        if best_position > 0:
            snippet = "..." + snippet
        if best_position + max_length < len(content):
            snippet = snippet + "..."
        
        return snippet
    
    async def _get_search_facets(self, organization_id: str, searchable_types: List[SearchableType],
                               filters: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Get search facets for filtering."""
        
        # Get content type facets
        content_types = self.db.query(
            SearchIndex.metadata["content_type"].astext.label("content_type"),
            func.count().label("count")
        ).filter(
            SearchIndex.organization_id == organization_id,
            SearchIndex.searchable_type.in_(searchable_types),
            SearchIndex.is_active == True
        ).group_by(SearchIndex.metadata["content_type"].astext).all()
        
        return {
            "content_types": [{"value": ct[0], "count": ct[1]} for ct in content_types if ct[0]]
        }
    
    async def _track_search_query(self, user_id: str, organization_id: str, query_text: str,
                                filters: Optional[Dict[str, Any]], results_count: int,
                                response_time_ms: int):
        """Track search query for analytics."""
        
        search_query = SearchQuery(
            user_id=user_id,
            organization_id=organization_id,
            query_text=query_text,
            filters=filters,
            results_count=results_count,
            response_time_ms=response_time_ms
        )
        
        self.db.add(search_query)
        self.db.commit()
        
        track_business_event("search_performed", {
            "user_id": user_id,
            "organization_id": organization_id,
            "query_length": len(query_text),
            "results_count": results_count,
            "response_time_ms": response_time_ms
        })
```

## 3. Search API Integration

### Enhanced Project API with Search
```python
# File: backend/app/api/v1/endpoints/projects.py (additions)
from app.services.enhanced_search_service import EnhancedSearchService

@router.get("/search")
async def search_projects(
    q: str = Query(..., min_length=2, description="Search query"),
    filters: Optional[Dict[str, Any]] = None,
    pagination: PaginationParams = Depends(),
    db: Session = Depends(get_db),
    current_user: UserModel = Depends(get_current_verified_user)
):
    """Search projects within organization."""
    search_service = EnhancedSearchService(db)
    
    try:
        results = await search_service.search_projects(
            query_text=q,
            organization_id=str(current_user.organization_id),
            filters=filters,
            pagination=pagination
        )
        
        return APISuccessResponse(data=results)
        
    except ValidationError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e)
        )
```

## 4. Automatic Content Indexing

### Content Indexing Hooks
```python
# File: backend/app/services/project_service.py (additions)
from app.services.enhanced_search_service import EnhancedSearchService
from app.models.search_index import SearchableType

class ProjectService:
    def __init__(self, db: Session):
        self.db = db
        self.search_service = EnhancedSearchService(db)
    
    async def create_project(self, project_data: ProjectCreate, user_id: str, organization_id: str) -> Project:
        """Enhanced project creation with automatic indexing."""
        try:
            # ... existing project creation logic ...
            
            # Index the new project for search
            await self.search_service.index_content(
                searchable_type=SearchableType.PROJECT,
                searchable_id=str(new_project.id),
                organization_id=organization_id
            )
            
            return new_project
            
        except Exception as e:
            self.db.rollback()
            raise
    
    async def update_project(self, project_id: str, project_data: ProjectUpdate, 
                           organization_id: str, user_id: str) -> Optional[Project]:
        """Enhanced project update with search reindexing."""
        try:
            # ... existing update logic ...
            
            # Reindex the updated project
            await self.search_service.index_content(
                searchable_type=SearchableType.PROJECT,
                searchable_id=project_id,
                organization_id=organization_id,
                force_reindex=True
            )
            
            return updated_project
            
        except Exception as e:
            self.db.rollback()
            raise
```

## 5. Search Configuration

### Environment Configuration
```python
# File: backend/app/core/config.py (additions)
class Settings(BaseSettings):
    # ... existing settings ...
    
    # Search Configuration
    SEARCH_ENABLED: bool = True
    SEARCH_MAX_RESULTS_PER_PAGE: int = 50
    SEARCH_DEFAULT_RESULTS_PER_PAGE: int = 20
    SEARCH_SNIPPET_MAX_LENGTH: int = 200
    SEARCH_AUTOCOMPLETE_MIN_LENGTH: int = 2
    SEARCH_AUTOCOMPLETE_MAX_SUGGESTIONS: int = 10
    
    # Full-text search configuration
    SEARCH_LANGUAGE: str = "english"  # PostgreSQL text search language
    SEARCH_REINDEX_BATCH_SIZE: int = 100
```

## 6. Search Maintenance Tasks

### Celery Tasks for Search Maintenance
```python
# File: backend/app/tasks/search_tasks.py
from celery import shared_task
from sqlalchemy.orm import Session

from app.core.database import SessionLocal
from app.services.enhanced_search_service import EnhancedSearchService
from app.models.search_index import SearchIndex, SearchableType

@shared_task
def reindex_all_content():
    """Reindex all searchable content (maintenance task)."""
    db: Session = SessionLocal()
    
    try:
        search_service = EnhancedSearchService(db)
        
        # Get all search index entries
        search_entries = db.query(SearchIndex).all()
        
        reindexed_count = 0
        failed_count = 0
        
        for entry in search_entries:
            try:
                success = await search_service.index_content(
                    searchable_type=entry.searchable_type,
                    searchable_id=entry.searchable_id,
                    organization_id=entry.organization_id,
                    force_reindex=True
                )
                
                if success:
                    reindexed_count += 1
                else:
                    failed_count += 1
                    
            except Exception as e:
                logger.error(f"Failed to reindex {entry.searchable_type}:{entry.searchable_id}: {e}")
                failed_count += 1
        
        logger.info(f"Reindexing completed: {reindexed_count} successful, {failed_count} failed")
        
    except Exception as e:
        logger.error(f"Reindexing task failed: {e}")
    finally:
        db.close()

@shared_task
def cleanup_old_search_queries(days_old: int = 30):
    """Clean up old search queries for analytics."""
    db: Session = SessionLocal()
    
    try:
        from datetime import datetime, timedelta
        cutoff_date = datetime.utcnow() - timedelta(days=days_old)
        
        deleted_count = db.query(SearchQuery).filter(
            SearchQuery.created_at < cutoff_date
        ).delete()
        
        db.commit()
        logger.info(f"Cleaned up {deleted_count} old search queries")
        
    except Exception as e:
        logger.error(f"Search query cleanup failed: {e}")
    finally:
        db.close()
```

## 7. Next Steps

This Search Integration Implementation provides:
- ✅ **Comprehensive search** across projects, content, and scan results
- ✅ **Full-text search** with PostgreSQL TSVECTOR indexing
- ✅ **Faceted filtering** for precise result refinement
- ✅ **Automatic indexing** when content is created or updated
- ✅ **Performance monitoring** with Sentry integration
- ✅ **Search analytics** for user behavior insights

**Next Implementation**: Complete notification system for user engagement to finish all critical MVP gaps.
