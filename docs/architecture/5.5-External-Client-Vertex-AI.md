Phase: 5 - DCS Engine & Scan Results
Part: 5.5
Title: Enterprise Google Vertex AI Client
Depends On: 5.4-External-Client-Google-PageSpeed.md
Objective: To implement a secure, reliable, and production-ready API client for the Google Vertex AI service (specifically the Gemini models). This client will handle complex interactions with the AI, including prompt engineering, response parsing, managing content safety settings, and resilient error handling.
1. Core Principle: Abstracting AI Complexity
Interacting with a powerful Large Language Model (LLM) like Gemini involves more than just sending a text prompt. It requires careful prompt engineering, managing safety settings, handling different response formats, and parsing the output. The VertexAIClient will abstract all this complexity away, providing simple, task-oriented methods like analyze_seo_data or generate_meta_tags to the rest of the application.
2. Dependency Installation
This client requires the official Google Cloud AI Platform library.
File to Modify: backend/requirements.txt
Content to Add:
google-cloud-aiplatform>=1.40.0

Installation Command (run in backend directory's venv):
pip install "google-cloud-aiplatform>=1.40.0"

3. VertexAIClient Implementation
This class is a standalone, resilient service that implements the necessary methods for interacting with the Vertex AI Gemini models. It does not inherit from our EnterpriseAPIClient because the Google SDK manages its own resilience and authentication.
File Location: backend/app/services/api_clients/vertexai_client.py
File Content:
import logging
import json
from typing import Dict, Any, Optional

import vertexai
from vertexai.generative_models import GenerativeModel, GenerationConfig, HarmCategory, SafetySetting
from google.api_core import exceptions as google_exceptions
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from app.core.config import Settings

logger = logging.getLogger(__name__)

# Define which specific Google API exceptions should trigger a retry
def is_vertex_retryable_exception(exception) -> bool:
    return isinstance(exception, (
        google_exceptions.ResourceExhausted, # e.g., Rate limit exceeded
        google_exceptions.ServiceUnavailable,
        google_exceptions.InternalServerError,
        google_exceptions.Aborted,
    ))

class VertexAIClient:
    """A standalone, resilient client for Google Vertex AI (Gemini) services."""

    def __init__(self, settings: Settings):
        self.settings = settings
        if not self.settings.GCP_PROJECT_ID:
            raise ValueError("GCP_PROJECT_ID must be configured for Vertex AI.")

        try:
            vertexai.init(project=self.settings.GCP_PROJECT_ID, location=self.settings.VERTEX_AI_LOCATION)
            self.model = GenerativeModel("gemini-1.5-pro-preview-0409") # Using a specific, powerful model
            logger.info("Vertex AI client initialized successfully.")
        except Exception as e:
            logger.critical(f"Failed to initialize Vertex AI SDK: {e}", exc_info=True)
            raise

    @retry(
        wait=wait_exponential(multiplier=1, min=2, max=60),
        stop=stop_after_attempt(5),
        retry=retry_if_exception_type(is_vertex_retryable_exception),
        before_sleep=lambda retry_state: logger.warning(
            f"Retrying Vertex AI call due to {retry_state.outcome.exception()}, "
            f"attempt {retry_state.attempt_number}"
        )
    )
    async def _generate_with_resilience(self, prompt: str) -> str:
        """Wraps the SDK call in Tenacity retry logic and returns the raw text response."""
        response = self.model.generate_content(
            [prompt],
            generation_config=self._get_generation_config(),
            safety_settings=self._get_safety_settings(),
            request_options={"timeout": 120} # Set a 2-minute timeout for the API call
        )
        return response.text

    async def analyze_seo_data(self, collected_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Analyzes a bundle of SEO data using the Gemini model.
        
        Args:
            collected_data (Dict[str, Any]): A dictionary containing data from other scans.
            
        Returns:
            Optional[Dict[str, Any]]: A structured JSON summary from the AI, or None on failure.
        """
        prompt = self._construct_seo_analysis_prompt(collected_data)
        
        try:
            raw_response_text = await self._generate_with_resilience(prompt)
            return self._parse_json_from_response(raw_response_text)
        except Exception as e:
            logger.error(f"Vertex AI analysis failed after all retries: {e}", exc_info=True)
            # The orchestrator will handle the missing data.
            return None

    def _construct_seo_analysis_prompt(self, collected_data: Dict[str, Any]) -> str:
        """Constructs a detailed, structured prompt for the Gemini model to analyze SEO data."""
        # A good prompt is specific and tells the model exactly what to output.
        prompt = f"""
        You are an expert SEO analyst. Analyze the following JSON data from a website scan and provide a concise, actionable summary.

        Scan Data:
        ```json
        {json.dumps(collected_data, indent=2)}
        ```

        Based on the data, perform the following tasks:
        1.  Provide a 1-2 sentence "Executive Summary" of the website's overall SEO health.
        2.  Identify the top 3 most critical issues that need immediate attention.
        3.  Calculate an "AI Citation Score" (an integer from 0-40) based on the presence and quality of structured data, robots.txt AI rules, and overall content friendliness.

        Return your response ONLY as a valid JSON object with the following structure:
        {{
          "executive_summary": "string",
          "critical_issues": ["string issue 1", "string issue 2", "string issue 3"],
          "ai_citation_score": integer
        }}
        """
        return prompt

    def _get_generation_config(self) -> GenerationConfig:
        """Returns a consistent generation configuration for the Gemini model to ensure predictable output."""
        return GenerationConfig(
            temperature=0.2, # Lower temperature for more deterministic, analytical tasks
            max_output_tokens=2048,
            # Ensure JSON output format if supported by the model version
        )

    def _get_safety_settings(self) -> Dict[HarmCategory, SafetySetting.HarmBlockThreshold]:
        """Returns restrictive safety settings to prevent harmful content generation."""
        return {
            HarmCategory.HARM_CATEGORY_HARASSMENT: SafetySetting.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: SafetySetting.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: SafetySetting.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: SafetySetting.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,
        }

    def _parse_json_from_response(self, response_text: str) -> Optional[Dict[str, Any]]:
        """Safely parses a JSON object from the AI's text response."""
        try:
            # Clean up common markdown formatting from the response
            response_text = response_text.strip().replace("```json", "").replace("```", "")
            return json.loads(response_text)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON response from Vertex AI: {e}. Response was: {response_text}")
            return None

4. Next Steps
We have now built all the necessary external API clients. Each client is resilient, secure, and abstracts the complexities of its specific third-party service. With these tools in our arsenal, we are ready to build the services that will use them.
Next File: 5.7-Backend-Project-Memory-Service.md