Phase: 6 - Content & Analysis Suite
Part: 6.14
Title: Backend Data Export Service
Depends On: 6.1-Backend-Unified-AI-Service.md, 5.12-Backend-Asset-Storage-Service.md
Objective: To implement a comprehensive data export service that enables enterprise users to export their projects, scan results, generated content, and analytics data in multiple formats. This service ensures data portability, compliance with data protection regulations, and supports business intelligence workflows.

## 1. Core Principles

**Format Flexibility**: Support multiple export formats (CSV, JSON, PDF, Excel) based on data type and user needs.
**Security First**: All exports respect organization boundaries and user permissions with audit trails.
**Async Processing**: Large exports are processed asynchronously to avoid blocking the API.
**Compliance Ready**: Support for GDPR data portability requirements and enterprise audit needs.

## 2. Data Export Models

### Export Request Model
```python
# File: backend/app/models/export_request.py
from sqlalchemy import Column, String, Text, DateTime, Enum, ForeignKey, Integer, JSON, Boolean
from sqlalchemy.orm import relationship
from app.models.base import Base, BaseMixin
import enum

class ExportType(str, enum.Enum):
    PROJECTS_SUMMARY = "PROJECTS_SUMMARY"
    PROJECT_DETAILED = "PROJECT_DETAILED"
    SCAN_RESULTS = "SCAN_RESULTS"
    SCAN_HISTORY = "SCAN_HISTORY"
    GENERATED_CONTENT = "GENERATED_CONTENT"
    CONTENT_ANALYTICS = "CONTENT_ANALYTICS"
    USAGE_ANALYTICS = "USAGE_ANALYTICS"
    CREDIT_USAGE_REPORT = "CREDIT_USAGE_REPORT"
    USER_DATA_EXPORT = "USER_DATA_EXPORT"  # GDPR compliance
    AUDIT_LOG_EXPORT = "AUDIT_LOG_EXPORT"

class ExportFormat(str, enum.Enum):
    CSV = "CSV"
    JSON = "JSON"
    EXCEL = "EXCEL"
    PDF = "PDF"

class ExportStatus(str, enum.Enum):
    PENDING = "PENDING"
    PROCESSING = "PROCESSING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
    EXPIRED = "EXPIRED"

class ExportRequest(Base, BaseMixin):
    __tablename__ = 'export_requests'
    
    export_type = Column(Enum(ExportType), nullable=False)
    export_format = Column(Enum(ExportFormat), nullable=False)
    
    user_id = Column(ForeignKey('users.id'), nullable=False)
    organization_id = Column(ForeignKey('organizations.id'), nullable=False)
    
    filters = Column(JSON, nullable=True)  # Date ranges, project IDs, etc.
    parameters = Column(JSON, nullable=True)  # Format-specific parameters
    
    status = Column(Enum(ExportStatus), default=ExportStatus.PENDING)
    progress_percentage = Column(Integer, default=0)
    
    file_path = Column(String(500), nullable=True)
    file_size = Column(Integer, nullable=True)
    download_count = Column(Integer, default=0)
    
    started_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)
    expires_at = Column(DateTime, nullable=False)  # Auto-cleanup after 7 days
    
    error_message = Column(Text, nullable=True)
    retry_count = Column(Integer, default=0)
    
    user = relationship('User', back_populates='export_requests')
    organization = relationship('Organization')
```

## 3. Data Export Service Implementation

```python
# File: backend/app/services/data_export_service.py
import logging
import csv
import json
import io
from typing import Dict, Any, Optional, List
from sqlalchemy.orm import Session
from datetime import datetime, timedelta

from app.core.config import settings
from app.models.export_request import ExportRequest, ExportType, ExportFormat, ExportStatus
from app.models.project import Project
from app.models.scan_result import ScanResult
from app.models.credit_ledger import CreditLedger
from app.services.asset_storage_service import AssetStorageService
from app.tasks.export_tasks import process_export_async

logger = logging.getLogger(__name__)

class DataExportService:
    """
    Comprehensive data export service supporting multiple formats and data types.
    """
    
    def __init__(self, db: Session):
        self.db = db
        self.asset_service = AssetStorageService(db)
        self.max_rows_sync = 1000  # Process synchronously if under this limit
        self.max_rows_total = 100000  # Hard limit for any export
    
    async def request_export(self, export_type: ExportType, export_format: ExportFormat,
                            user_id: str, organization_id: str,
                            filters: Optional[Dict[str, Any]] = None,
                            parameters: Optional[Dict[str, Any]] = None) -> ExportRequest:
        """Create a new export request."""
        if not self._check_export_permission(export_type, user_id, organization_id):
            raise PermissionError(f"User does not have permission for {export_type}")
        
        expires_at = datetime.utcnow() + timedelta(days=7)
        
        export_request = ExportRequest(
            export_type=export_type,
            export_format=export_format,
            user_id=user_id,
            organization_id=organization_id,
            filters=filters or {},
            parameters=parameters or {},
            expires_at=expires_at
        )
        
        self.db.add(export_request)
        self.db.commit()
        self.db.refresh(export_request)
        
        estimated_rows = self._estimate_export_size(export_type, organization_id, filters)
        
        if estimated_rows > self.max_rows_total:
            export_request.status = ExportStatus.FAILED
            export_request.error_message = f"Export too large: {estimated_rows} rows exceeds limit"
            self.db.commit()
            return export_request
        
        if estimated_rows <= self.max_rows_sync:
            try:
                await self._process_export_sync(export_request)
            except Exception as e:
                export_request.status = ExportStatus.FAILED
                export_request.error_message = str(e)
                self.db.commit()
        else:
            process_export_async.delay(str(export_request.id))
        
        return export_request
    
    async def _process_export_sync(self, export_request: ExportRequest):
        """Process export synchronously for small datasets."""
        export_request.status = ExportStatus.PROCESSING
        export_request.started_at = datetime.utcnow()
        self.db.commit()
        
        try:
            data = self._extract_data(export_request)
            file_content = self._generate_file(data, export_request.export_format, export_request.parameters)
            
            filename = self._generate_filename(export_request)
            file_obj = io.BytesIO(file_content)
            
            asset = await self.asset_service.upload_asset(
                file_content=file_obj,
                filename=filename,
                asset_type=AssetType.EXPORT_FILE,
                user_id=export_request.user_id,
                organization_id=export_request.organization_id,
                is_public=False
            )
            
            if asset:
                export_request.file_path = asset.storage_path
                export_request.file_size = len(file_content)
                export_request.status = ExportStatus.COMPLETED
                export_request.completed_at = datetime.utcnow()
                export_request.progress_percentage = 100
            else:
                raise Exception("Failed to save export file")
            
        except Exception as e:
            export_request.status = ExportStatus.FAILED
            export_request.error_message = str(e)
            logger.error(f"Export failed: {e}")
        
        self.db.commit()
    
    def _extract_data(self, export_request: ExportRequest) -> List[Dict[str, Any]]:
        """Extract data based on export type and filters."""
        filters = export_request.filters
        org_id = export_request.organization_id
        
        if export_request.export_type == ExportType.PROJECTS_SUMMARY:
            return self._extract_projects_summary(org_id, filters)
        elif export_request.export_type == ExportType.SCAN_RESULTS:
            return self._extract_scan_results(org_id, filters)
        elif export_request.export_type == ExportType.CREDIT_USAGE_REPORT:
            return self._extract_credit_usage(org_id, filters)
        elif export_request.export_type == ExportType.USER_DATA_EXPORT:
            return self._extract_user_data(export_request.user_id, org_id)
        else:
            raise ValueError(f"Unsupported export type: {export_request.export_type}")
    
    def _extract_projects_summary(self, org_id: str, filters: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract projects summary data."""
        query = self.db.query(Project).filter(Project.organization_id == org_id)
        
        if 'date_from' in filters:
            query = query.filter(Project.created_at >= filters['date_from'])
        if 'date_to' in filters:
            query = query.filter(Project.created_at <= filters['date_to'])
        
        projects = query.all()
        
        return [{
            'project_id': str(project.id),
            'name': project.name,
            'url': project.url,
            'description': project.description,
            'created_at': project.created_at.isoformat(),
            'updated_at': project.updated_at.isoformat(),
            'scan_count': len(project.audits),
            'last_scan_date': max([audit.created_at for audit in project.audits]).isoformat() if project.audits else None
        } for project in projects]
    
    def _extract_scan_results(self, org_id: str, filters: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract scan results data."""
        query = self.db.query(ScanResult).join(
            ScanResult.audit
        ).join(
            Audit.project
        ).filter(Project.organization_id == org_id)
        
        if 'date_from' in filters:
            query = query.filter(ScanResult.created_at >= filters['date_from'])
        if 'date_to' in filters:
            query = query.filter(ScanResult.created_at <= filters['date_to'])
        if 'project_ids' in filters:
            query = query.filter(Project.id.in_(filters['project_ids']))
        
        scan_results = query.all()
        
        return [{
            'scan_id': str(scan.id),
            'project_name': scan.audit.project.name,
            'project_url': scan.audit.project.url,
            'audit_type': scan.audit.audit_type,
            'dcs_score': scan.dcs_score,
            'ai_citation_score': scan.ai_citation_score,
            'scan_date': scan.created_at.isoformat(),
            'credits_consumed': scan.audit.credits_consumed,
            'status': scan.audit.status
        } for scan in scan_results]
    
    def _extract_credit_usage(self, org_id: str, filters: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract credit usage data."""
        query = self.db.query(CreditLedger).filter(CreditLedger.organization_id == org_id)
        
        if 'date_from' in filters:
            query = query.filter(CreditLedger.created_at >= filters['date_from'])
        if 'date_to' in filters:
            query = query.filter(CreditLedger.created_at <= filters['date_to'])
        
        credit_entries = query.order_by(CreditLedger.created_at.desc()).all()
        
        return [{
            'transaction_id': str(entry.id),
            'amount': entry.amount,
            'balance_after': entry.balance_after,
            'description': entry.description,
            'transaction_type': 'credit' if entry.amount > 0 else 'debit',
            'created_at': entry.created_at.isoformat(),
            'reference_id': entry.reference_id
        } for entry in credit_entries]
    
    def _extract_user_data(self, user_id: str, org_id: str) -> List[Dict[str, Any]]:
        """Extract all user data for GDPR compliance."""
        user = self.db.query(User).filter_by(id=user_id).first()
        if not user:
            return []
        
        user_data = [{
            'data_type': 'user_profile',
            'user_id': str(user.id),
            'email': user.email,
            'full_name': user.full_name,
            'created_at': user.created_at.isoformat(),
            'last_login_at': user.last_login_at.isoformat() if user.last_login_at else None,
            'is_verified': user.is_verified
        }]
        
        projects = self.db.query(Project).filter_by(user_id=user_id).all()
        for project in projects:
            user_data.append({
                'data_type': 'project',
                'project_id': str(project.id),
                'name': project.name,
                'url': project.url,
                'description': project.description,
                'created_at': project.created_at.isoformat()
            })
        
        return user_data
    
    def _generate_file(self, data: List[Dict[str, Any]], format: ExportFormat, 
                      parameters: Optional[Dict[str, Any]] = None) -> bytes:
        """Generate file content based on format."""
        if format == ExportFormat.CSV:
            return self._generate_csv(data)
        elif format == ExportFormat.JSON:
            return self._generate_json(data)
        elif format == ExportFormat.EXCEL:
            return self._generate_excel(data)
        elif format == ExportFormat.PDF:
            return self._generate_pdf(data, parameters)
        else:
            raise ValueError(f"Unsupported export format: {format}")
    
    def _generate_csv(self, data: List[Dict[str, Any]]) -> bytes:
        """Generate CSV file content."""
        if not data:
            return b""
        
        output = io.StringIO()
        writer = csv.DictWriter(output, fieldnames=data[0].keys())
        writer.writeheader()
        writer.writerows(data)
        
        return output.getvalue().encode('utf-8')
    
    def _generate_json(self, data: List[Dict[str, Any]]) -> bytes:
        """Generate JSON file content."""
        return json.dumps(data, indent=2, default=str).encode('utf-8')
    
    def _generate_excel(self, data: List[Dict[str, Any]]) -> bytes:
        """Generate Excel file content."""
        try:
            import pandas as pd
            
            df = pd.DataFrame(data)
            output = io.BytesIO()
            
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='Export', index=False)
            
            return output.getvalue()
        except ImportError:
            raise Exception("pandas and openpyxl required for Excel export")
    
    def _generate_pdf(self, data: List[Dict[str, Any]], parameters: Optional[Dict[str, Any]] = None) -> bytes:
        """Generate PDF file content."""
        # Simplified PDF generation - in production use reportlab
        content = "PDF Export\n\n"
        for item in data[:50]:  # Limit for PDF
            content += f"{json.dumps(item, indent=2)}\n\n"
        
        return content.encode('utf-8')
    
    def _generate_filename(self, export_request: ExportRequest) -> str:
        """Generate filename for export."""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        extension = export_request.export_format.lower()
        
        return f"{export_request.export_type.lower()}_{timestamp}.{extension}"
    
    def _estimate_export_size(self, export_type: ExportType, org_id: str, 
                             filters: Optional[Dict[str, Any]] = None) -> int:
        """Estimate number of rows for export."""
        if export_type == ExportType.PROJECTS_SUMMARY:
            return self.db.query(Project).filter(Project.organization_id == org_id).count()
        elif export_type == ExportType.SCAN_RESULTS:
            return self.db.query(ScanResult).join(ScanResult.audit).join(Audit.project).filter(
                Project.organization_id == org_id
            ).count()
        else:
            return 1000  # Default estimate
    
    def _check_export_permission(self, export_type: ExportType, user_id: str, org_id: str) -> bool:
        """Check if user has permission for export type."""
        user = self.db.query(User).filter_by(id=user_id).first()
        
        admin_only_exports = [ExportType.AUDIT_LOG_EXPORT, ExportType.USAGE_ANALYTICS]
        if export_type in admin_only_exports:
            return user and user.is_admin
        
        if export_type == ExportType.USER_DATA_EXPORT:
            return True  # Users can always export their own data
        
        return user and user.organization_id == org_id
    
    def get_export_status(self, export_id: str, user_id: str) -> Optional[ExportRequest]:
        """Get export request status."""
        return self.db.query(ExportRequest).filter(
            ExportRequest.id == export_id,
            ExportRequest.user_id == user_id
        ).first()
    
    def get_user_exports(self, user_id: str, limit: int = 50) -> List[ExportRequest]:
        """Get user's export history."""
        return self.db.query(ExportRequest).filter(
            ExportRequest.user_id == user_id
        ).order_by(ExportRequest.created_at.desc()).limit(limit).all()
    
    async def download_export(self, export_id: str, user_id: str) -> Optional[bytes]:
        """Download completed export file."""
        export_request = self.get_export_status(export_id, user_id)
        
        if not export_request or export_request.status != ExportStatus.COMPLETED:
            return None
        
        if not export_request.file_path:
            return None
        
        content = await self.asset_service.storage_provider.download_file(export_request.file_path)
        
        if content:
            export_request.download_count += 1
            self.db.commit()
        
        return content
    
    def cleanup_expired_exports(self):
        """Clean up expired export files."""
        expired_exports = self.db.query(ExportRequest).filter(
            ExportRequest.expires_at < datetime.utcnow(),
            ExportRequest.status == ExportStatus.COMPLETED
        ).all()
        
        for export in expired_exports:
            if export.file_path:
                try:
                    self.asset_service.storage_provider.delete_file(export.file_path)
                except Exception as e:
                    logger.error(f"Failed to delete export file {export.file_path}: {e}")
            
            export.status = ExportStatus.EXPIRED
            export.file_path = None
        
        self.db.commit()
        logger.info(f"Cleaned up {len(expired_exports)} expired exports")
```

## 4. Export API Endpoints

```python
# File: backend/app/api/v1/endpoints/exports.py
import logging
from fastapi import APIRouter, Depends, HTTPException, status, Response
from sqlalchemy.orm import Session
from typing import List, Optional, Dict, Any

from app.core.database import get_db
from app.api.v1.dependencies.auth_deps import get_current_active_user
from app.models.user import User as UserModel
from app.models.export_request import ExportType, ExportFormat, ExportStatus
from app.services.data_export_service import DataExportService
from app.schemas.common_schemas import APISuccessResponse

logger = logging.getLogger(__name__)
router = APIRouter()

@router.post("/request", status_code=status.HTTP_201_CREATED)
async def request_export(
    export_type: ExportType,
    export_format: ExportFormat,
    filters: Optional[Dict[str, Any]] = None,
    parameters: Optional[Dict[str, Any]] = None,
    db: Session = Depends(get_db),
    current_user: UserModel = Depends(get_current_active_user)
):
    """Request a new data export."""
    export_service = DataExportService(db)
    
    try:
        export_request = await export_service.request_export(
            export_type=export_type,
            export_format=export_format,
            user_id=str(current_user.id),
            organization_id=str(current_user.organization_id),
            filters=filters,
            parameters=parameters
        )
        
        return APISuccessResponse(data={
            'export_id': str(export_request.id),
            'status': export_request.status,
            'estimated_completion': export_request.expires_at.isoformat(),
            'progress_percentage': export_request.progress_percentage
        })
        
    except PermissionError as e:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"Export request failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to create export request"
        )

@router.get("/status/{export_id}")
async def get_export_status(
    export_id: str,
    db: Session = Depends(get_db),
    current_user: UserModel = Depends(get_current_active_user)
):
    """Get export status."""
    export_service = DataExportService(db)
    
    export_request = export_service.get_export_status(export_id, str(current_user.id))
    
    if not export_request:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Export not found"
        )
    
    return APISuccessResponse(data={
        'export_id': str(export_request.id),
        'export_type': export_request.export_type,
        'export_format': export_request.export_format,
        'status': export_request.status,
        'progress_percentage': export_request.progress_percentage,
        'file_size': export_request.file_size,
        'created_at': export_request.created_at.isoformat(),
        'completed_at': export_request.completed_at.isoformat() if export_request.completed_at else None,
        'expires_at': export_request.expires_at.isoformat(),
        'download_count': export_request.download_count,
        'error_message': export_request.error_message
    })

@router.get("/download/{export_id}")
async def download_export(
    export_id: str,
    db: Session = Depends(get_db),
    current_user: UserModel = Depends(get_current_active_user)
):
    """Download completed export file."""
    export_service = DataExportService(db)
    
    export_request = export_service.get_export_status(export_id, str(current_user.id))
    
    if not export_request:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Export not found"
        )
    
    if export_request.status != ExportStatus.COMPLETED:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Export not ready for download. Status: {export_request.status}"
        )
    
    content = await export_service.download_export(export_id, str(current_user.id))
    
    if not content:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Export file not found"
        )
    
    content_types = {
        ExportFormat.CSV: "text/csv",
        ExportFormat.JSON: "application/json",
        ExportFormat.EXCEL: "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        ExportFormat.PDF: "application/pdf"
    }
    
    content_type = content_types.get(export_request.export_format, "application/octet-stream")
    filename = f"{export_request.export_type.lower()}.{export_request.export_format.lower()}"
    
    return Response(
        content=content,
        media_type=content_type,
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )

@router.get("/history", response_model=APISuccessResponse[List[dict]])
async def get_export_history(
    db: Session = Depends(get_db),
    current_user: UserModel = Depends(get_current_active_user)
):
    """Get user's export history."""
    export_service = DataExportService(db)
    
    exports = export_service.get_user_exports(str(current_user.id))
    
    return APISuccessResponse(data=[{
        'export_id': str(export.id),
        'export_type': export.export_type,
        'export_format': export.export_format,
        'status': export.status,
        'file_size': export.file_size,
        'created_at': export.created_at.isoformat(),
        'completed_at': export.completed_at.isoformat() if export.completed_at else None,
        'expires_at': export.expires_at.isoformat(),
        'download_count': export.download_count
    } for export in exports])
```

## 5. Configuration Updates

```python
# File: backend/app/core/config.py (additions)
class Settings(BaseSettings):
    # ... existing settings ...
    
    # Data Export Configuration
    EXPORT_ENABLED: bool = True
    EXPORT_MAX_ROWS_SYNC: int = 1000
    EXPORT_MAX_ROWS_TOTAL: int = 100000
    EXPORT_RETENTION_DAYS: int = 7
    
    # Export Format Support
    EXPORT_EXCEL_ENABLED: bool = True  # Requires pandas, openpyxl
    EXPORT_PDF_ENABLED: bool = True    # Requires reportlab
```

## 6. Async Export Task

```python
# File: backend/app/tasks/export_tasks.py
import logging
from celery import shared_task
from sqlalchemy.orm import Session

from app.core.database import SessionLocal
from app.models.export_request import ExportRequest, ExportStatus
from app.services.data_export_service import DataExportService

logger = logging.getLogger(__name__)

@shared_task(bind=True, max_retries=3, default_retry_delay=300)
def process_export_async(self, export_id: str):
    """
    Async task to process large export requests.
    """
    db: Session = SessionLocal()
    
    try:
        export_request = db.query(ExportRequest).filter_by(id=export_id).first()
        if not export_request:
            logger.error(f"Export request not found: {export_id}")
            return
        
        export_service = DataExportService(db)
        await export_service._process_export_sync(export_request)
        
        logger.info(f"Export completed successfully: {export_id}")
        
    except Exception as exc:
        logger.error(f"Export task failed: {exc}")
        if db.is_active:
            db.rollback()
        raise self.retry(exc=exc)
    finally:
        db.close()

@shared_task
def cleanup_expired_exports():
    """
    Periodic task to clean up expired export files.
    """
    db: Session = SessionLocal()
    
    try:
        export_service = DataExportService(db)
        export_service.cleanup_expired_exports()
        
    except Exception as e:
        logger.error(f"Export cleanup failed: {e}")
    finally:
        db.close()
```

## 7. Next Steps

This Data Export Service provides:
- ✅ **Multi-format exports** (CSV, JSON, Excel, PDF)
- ✅ **Async processing** for large datasets
- ✅ **GDPR compliance** with user data export
- ✅ **Security controls** with permission checks
- ✅ **Automatic cleanup** of expired files

**Next File**: Standardize API patterns across all endpoint files to ensure consistency.
